# 5. VERDIEPEN – Theoretische verdieping

**Leerdoel**
In deze module verdiep je je verder in de onderliggende theorie en concepten achter AI, zodat je een beter begrip ontwikkelt van de basis en de logica van AI-systemen.

---
De geboorte van AI heeft een aantal voorlopers maar voor deze introductie is dit niet relevant. Daarom starten we bij de geboorte van het idee van AI. 

**De geboorte van het idee (1950–1970)**

- **Alan Turing: "Kunnen machines denken?"**
De wortels van AI liggen in de jaren ’50. Alan Turing stelde in 1950 de beroemde vraag: “Kunnen machines denken?” Hij bedacht de Turingtest als criterium: als een mens in een gesprek niet kan onderscheiden of hij met een machine of een mens praat, is de machine intelligent.

- **De introductie van de term AI: de Dartmouth Workshop**
In 1956 kreeg het veld officieel een naam: tijdens de Dartmouth-conferentie werd de term Artificial Intelligence geïntroduceerd. Onderzoekers geloofden dat menselijke intelligentie in principe volledig te beschrijven was in regels en logica.

- **Het begin van de klassieke AI**
Dit leidde tot wat later GOFAI (Good Old-Fashioned AI) genoemd werd. Het draaide om symbolische systemen: kennis werd vastgelegd in regels (als A, dan B), en de computer volgde deze stap voor stap. In de jaren ’70 en ’80 werden deze systemen steeds groter en beter: zogenaamde expert systems konden bijvoorbeeld medische diagnoses stellen of technische problemen analyseren.

Maar GOFAI had duidelijke grenzen:

- Het was kwetsbaar: zodra een situatie buiten de regels viel, liep het systeem vast.

- Het was star: het leerde niet bij van ervaring, alles moest handmatig worden geprogrammeerd.

**De eerste golf van leren: Machine Learning (1980–2000)**

Onderzoekers beseften dat een systeem flexibeler zou moeten zijn: niet alles kan vooraf worden vastgelegd. Dit leidde tot de opkomst van Machine Learning (ML).

In plaats van regels te programmeren, wordt een systeem gevoed met data. Het model leert daaruit patronen herkennen: van simpele regressie en beslisbomen tot complexere statistische modellen.

Belangrijke verschuivingen:

- AI werd empirischer: niet meer alleen logica, maar ook wiskunde en kansberekening.

- Er kwamen neurale netwerken (geïnspireerd door de werking van hersenen), maar deze bleven beperkt door rekenkracht en gebrek aan data.

In deze periode ontstonden ook de eerste AI-winters. Verwachtingen waren hoog, maar de techniek leverde niet wat men hoopte. Toch werden in stilte de fundamenten gelegd voor de doorbraken van later.

**Neurale netwerken en de terugkeer van een oud idee (2000–2010)**

Neurale netwerken zijn eigenlijk een oud idee (al uit de jaren ’50), maar kregen rond de eeuwwisseling een tweede leven. Met snellere computers en grotere datasets bleek dat deze netwerken krachtiger waren dan gedacht.

Ze bestaan uit lagen van neuronen die signalen doorgeven en versterken of verzwakken.

Door training met data passen ze zich automatisch aan.

In tegenstelling tot GOFAI konden ze leren van ervaring.

Nog was hun schaal beperkt – tot rond 2010 een nieuwe doorbraak kwam: Deep Learning.

**Deep Learning en de grote doorbraak (2010–2018)**

Deep Learning is een vorm van machine learning die gebruikmaakt van neurale netwerken met veel lagen (deep neural networks). Dankzij GPU’s (grafische processors) en enorme hoeveelheden data werden ze ineens extreem krachtig.

Doorbraken volgden elkaar snel op:

- Beeldherkenning werd spectaculair beter (ImageNet-competitie, 2012).

- Spraakassistenten als Siri en Google Assistant kwamen op de markt.

- Vertaalmachines werden betrouwbaarder.

- Voor het eerst begon AI door te dringen in het dagelijks leven. Dit was de start van de huidige revolutie.

**Foundation Models, LLM’s en Generatieve AI (2018–nu)**

Een echte gamechanger kwam met de introductie van transformers (2017). Deze architectuur maakte het mogelijk om modellen op ongekende schaal te trainen.

Zo ontstonden de Foundation Models: enorme AI-systemen die op gigantische hoeveelheden tekst, beeld of code werden getraind. Voorbeelden: GPT (OpenAI), BERT (Google) en Stable Diffusion.

Binnen deze categorie vallen de Large Language Models (LLM’s), die specifiek met tekst werken.

Wat zijn LLM’s? Gigantische neurale netwerken die getraind zijn op enorme hoeveelheden tekst.

Hoe werken ze? Ze voorspellen steeds het volgende woord in een zin en leren zo vloeiende, coherente taal produceren.

Waarom belangrijk? Ze maken generatieve AI breed toegankelijk: van ChatGPT tot Gemini en Claude.

Kenmerken van foundation models en LLM’s:

- Ze zijn generiek: breed toepasbaar op allerlei taken.

- Ze zijn generatief: ze kunnen zelf nieuwe tekst, beelden of muziek produceren.

- Ze zijn fijn-afstelbaar: met beperkte extra training inzetbaar in specifieke domeinen.

Dit maakte de weg vrij voor AI die voor iedereen toegankelijk is, niet alleen voor onderzoekers of techbedrijven.

**Naar de toekomst: agents en AGI**

Vandaag werken onderzoekers aan AI-agents: systemen die niet alleen antwoorden geven, maar ook zelfstandig acties ondernemen, tools gebruiken en informatiebronnen raadplegen. Een voorbeeld is RAG (Retrieval-Augmented Generation): een techniek waarbij AI actuele informatie ophaalt en combineert met eigen kennis.

Toch bevinden we ons nog steeds in het tijdperk van narrow AI: systemen die uitblinken in specifieke taken, maar geen menselijk bewustzijn of algemene intelligentie hebben. De term AGI (Artificial General Intelligence) verwijst naar een toekomstig systeem dat wél over zulke brede capaciteiten beschikt.
En de overtreffende trap is ASI (Artificial Super Intelligence). In dit geval kan het systeem zichzelf verbeteren, zelfstandig beslissingen maken en valt het denken buiten het begrip van de mens.  

> "Superintelligentie kan onze grootste prestatie zijn, maar ook de laatste." – Stephen Hawking




























Filosofie achter AI: intelligentie, bewustzijn, creativiteit

Complexe modellen: deep learning, transformers

Taal en betekenis: hoe begrijpen machines taal?

AI en menselijk denken: overeenkomsten en verschillen

Debatten: singulariteit, autonomie, superintelligentie

---
Deep learning in detail (convolutional & recurrent netwerken, transformers)

Transfer learning

Embeddings

Large Language Models (LLM’s)

Token(s) en context window

Retrieval-Augmented Generation (RAG) – theorie

AI Agents – concept en werking

Cognitieve architecturen (redeneren, geheugen, planning)

Filosofie: bewustzijn, intelligentie, singulariteit

Symbolische AI vs. connectionistische AI