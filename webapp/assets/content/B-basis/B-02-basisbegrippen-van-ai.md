# BASISBEGRIPPEN VAN AI: de ontwikkeling van AI en de drijvende krachten

<br/>

**Datum:** augustus 2025

**Laatste wijziging:** 27 augustus 2025

**Leerdoel:**
- In deze module leer je de basibegrippen van AI.

**Voorkennis:** [B-01 AI beschrijven](B-01-ai-beschrijven)

**Leestijd:** <!--nog bepalen-->

**Inhoud:**

<br/>

## Inleiding

ChatGPT is voor veel mensen inmiddels een bekend begrip maar [veel mensen blijken ChatGPT en AI bijna gelijk te stellen.](https://www.researchgate.net/publication/375589046_Human_Perception_of_the_Sentience_of_ChatGPT_and_Artificial_Intelligence) 
<br/>
[Dr. Milan Milanovic](https://milan.milanovic.org/) benoemt ["dat "ChatGPT, 'AI' noemen hetzelfde is als een magnetron, 'koken'noemen."](https://newsletter.techworld-with-milan.com/p/chatgpt-is-not-ai).    

Deze afbeelding maakt duidelijk dat er veel verschillende kernbegrippen en stappen zijn tussen het algemene begrip AI en een tool als ChatGPT.

![Van AI naar ChatGPT](/img/ai-terminology.jpg)
*Bron: [AI for Everyone](https://medium.com/@wjleon/ai-for-everyone-demystifying-artificial-intelligence-d793d99583e9)*

De kernbegrippen gaan we in dit artikel verder toelichten. Dit doen we door ze in de tijdlijn van de AI-ontwikkeling te plaatsen. 
Door de begrippen in deze ontwikkeling te plaatsen, zie je beter wat ze betekenen, hoe ze zich tot elkaar verhouden en waarom sommige ideeën steeds opnieuw terugkomen. Dit helpt je begrijpen waarom AI-definities voortdurend verschuiven - wat vandaag als AI geldt, was dat gisteren misschien nog niet. 

Het maakt ook duidelijk dat AI niet alleen van deze tijd is maar al een geschiedenis heeft van 70 jaar! 

De beschrijving van de geschiedenis van AI door [Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_intelligence) en [Brittanica](https://www.britannica.com/science/history-of-artificial-intelligence/Connectionism) zijn als bron gebuikt om deze tijdlijn te beschrijven. Maar, let op: dit is geen beschrijving van de geschiedenis; de focus ligt op belangrijke de drijvende krachten die de ontwikkeling van AI mogelijk hebben gemaakt tot wat het vandaag de dag is. Deze drijvende krachten maken duidelijk wat de basisbegrippen zijn.

<br/>

## De ontwikkeling van AI en de drijvende krachten 

**1950–1970**

- **Het idee AI**
<br/>
De oorsprong van het idee achter AI wordt vaak toegeschreven aan [Alan Turing](referentie/begrippenlijst.md#alan-turing). In de jaren 1950 stelde Turing de vraag: "Kunnen machines denken?" en introduceerde hij de beroemde Turingtest als manier om te bepalen of een machine intelligent gedrag kan vertonen dat niet te onderscheiden is van dat van een mens. Hiermee legde hij de basis voor het denken over kunstmatige intelligentie.
    > ![Alan Turing](/img/Alan_Turing_(1951)_(crop).jpg)
    *Alan Turing, WikiMedia*
    > 
    > **📌 Alan Turing**
    > - Turing was een briljant wiskundige en grondlegger van de informatica (Turingmachine).
    > - Hij speelde een sleutelrol bij het kraken van de Enigma-code in WOII.
    > - Werd in 1952 veroordeeld vanwege zijn homoseksualiteit; kreeg pas in 2013 postuum gratie.
    > - De "Turing Award" is de belangrijkste prijs in de informatica.

- **Regelgebaseerd**
<br/>
Deze periode was de geboorte van de regelgebaseerde AI-methode. Logica en redeneringen worden weergegeven met symbolen en formele regels. Computers konden hierdoor 'redeneren' met regels die door mensen waren ingevoerd. Heel simpel gezegd: als je genoeg regels opschrijft, kan een machine intelligent gedrag vertonen.

- **Het idee van neurale netwerken**
<br/>
In die tijd was er de wens om het menselijk brein na te bootsen. Ons brein bestaat uit miljarden neuronen (zenuwcellen) die signalen doorgeven via verbindingen (synapsen). Elk neuron "vuurt" alleen wanneer het voldoende input krijgt van andere neuronen. Dit simpele principe inspireerde onderzoekers: als je dit in een computer kon nabouwen, zou de machine misschien ook kunnen leren. Later werd dit idee [neuraal netwerk](referentie/begrippenlijst.md#neuraal-netwerk) genoemd. 
<br/>
Een neuraal netwerk in de AI-context is daarom een systeem van “kunstmatige neuronen” die onderling verbonden zijn. Elke verbinding heeft een gewicht: een getal dat aangeeft hoe sterk een input meetelt. Door de gewichten aan te passen op basis van voorbeelden, kan een netwerk patronen herkennen – zoals letters, geluiden of afbeeldingen.

    > ![Biologische neuron en artificieel neuraal netwerk](/img/llustrative-image-of-artificial-neural-networks-with-their-working-mechanism-as-like.png)
    *Bron [ResearchGate](https://www.researchgate.net/figure/llustrative-image-of-artificial-neural-networks-with-their-working-mechanism-as-like_fig13_380136894)*

<br/>

- **De (single-layer) perceptron (1958)**
<br/>
  In 1958 ontwierp [Frank Rosenblatt](referentie/begrippenlijst.md#frank-rosenblatt) de (single-layer) [perceptron](referentie/begrippenlijst.md#perceptron), een van de eerste praktisch toepasbare [neurale netwerk](referentie/begrippenlijst.md#neuraal-netwerk)-modellen. Het bestond uit één laag artificiële neuronen en kon simpele patronen herkennen. Rosenblatt was optimistisch en voorspelde dat zulke netwerken in de toekomst zouden kunnen leren, beslissingen nemen en zelfs talen vertalen. 
  <br/>

  > **📌 De perceptron en de eerste AI-winter**  
  > Rosenblatt's perceptron leek veelbelovend, maar in 1969 publiceerden Marvin Minsky en Seymour Papert een vernietigend rapport. Ze bewezen dat perceptrons fundamentele beperkingen hadden: ze konden alleen lineair scheidbare patronen leren. Het simpele XOR-probleem (exclusieve OF) kon een perceptron niet oplossen. Dit rapport, plus de hoge kosten van computers in die tijd, leidde tot de eerste AI-winter: een periode van 10+ jaar waarin weinig geld naar AI-onderzoek ging. Het duurde tot de jaren '80 voordat neurale netwerken weer serieus werden genomen.

**1970–1980**

- **AI-winter**
<br/>
Verwachtingen werden niet waargemaakt. Er brak een periode aan waarin investeringen en enthousiasme voor AI sterk afnam. Zo'n periode wordt ook wel [AI-winter](referentie/begrippenlijst.md#ai-winter) genoemd. Er waren grofweg twee ‘winters’: rond 1973 (na het Lighthill‑rapport) en later circa 1987–1993 (na de hype rond expertsystemen).

**1980–2010** 

- **Expert Systems**
<br/>
Laat jaren ’70 en vooral in de jaren ’80 kwamen [expertsystemen](referentie/begrippenlijst.md#expertsysteem) op: programma’s die kennisbanken met logische regels gebruikten om beslissingen te nemen. Ze werden ingezet in bijvoorbeeld de medische wereld om diagnoses te ondersteunen. De beperkingen werden later duidelijk: systemen waren moeilijk te onderhouden en leerden niet zelf bij. 
<br/>

- **De opkomst van Machine Learning**
<br/>
  De symbolische AI liep vast op problemen zoals waarneming, leren en gezond verstand. Dit maakte duidelijk dat andere benaderingen nodig waren. Het connectionistische paradigma kwam tot bloei door een aantal doorbraken die leidden tot de heropleving van neurale netwerken en de opkomst van [machine learning](referentie/begrippenlijst.md#machine-learning). In dit paradigma worden [algoritmen](referentie/begrippenlijst.md#algoritme) gebruikt om modellen te trainen op data.

**2010–2017**

- **Doorbraak van Deep Learning**
<br>
De grote doorbraak kwam met [deep learning](referentie/begrippenlijst.md#deep-learning). Dankzij krachtige GPU’s en enorme hoeveelheden data konden veel diepere [neurale netwerken](referentie/begrippenlijst.md#neuraal-netwerk) worden getraind. Dit maakte plotseling toepassingen zoals spraakherkenning, beeldherkenning en vertaling bruikbaar in de praktijk.

In het volgende deel wordt toegelicht wat ervoor heeft gezorgd dat AI opeens in een stroomversnelling is gekomen. 

<br/>

## AI in de tegenwoordige tijd

- **Transformers**
In 2017 werd een belangrijke stap gezet in de ontwikkeling van AI met de introductie van de [Transformer](referentie/begrippenlijst.md#transformer)-architectuur. Deze technologie maakte het mogelijk om AI-modellen te bouwen die veel groter en krachtiger zijn dan ooit tevoren. Deze modellen, met miljarden [parameters](referentie/begrippenlijst.md#parameters), kunnen enorme hoeveelheden tekst en afbeeldingen verwerken. 

- **Generatieve AI**
Generatieve AI kwam vanaf 2017 in een stroomversnelling dankzij de transformer-architectuur. Hierdoor konden AI-systemen niet alleen data analyseren, maar ook zelf nieuwe teksten, beelden of andere content maken. Generatieve AI markeert een nieuw tijdperk waarin AI niet alleen herkent, maar ook creëert.
<br/>

- **GPT's**
<br/>

**GPT's (Generative Pre-trained Transformers)**  
GPT-modellen zijn een bekend voorbeeld van generatieve AI. Ze werden geïntroduceerd door OpenAI in 2018 en zijn gebaseerd op de transformer-architectuur. In plaats van voor één taak te worden getraind, leren GPT's eerst van enorme hoeveelheden tekst (pre-training) en kunnen daarna worden aangepast voor specifieke toepassingen (fine-tuning). 

Met elke nieuwe versie (zoals GPT-2, GPT-3, GPT-4) werden de modellen groter en krachtiger, waardoor ze steeds beter in staat zijn om natuurlijke taal te begrijpen en te genereren. GPT's vormen de basis van veel moderne AI-toepassingen, zoals chatbots en tekstgenerators.

- **Foundantion models**

- **LLM's**

- **MLLM's**



- **Transformers, LLM's, Generatieve AI en Foundation Models**  
Dit leidde tot de ontwikkeling van Foundation Models en [Large Language Models (LLM’s)](referentie/begrippenlijst.md#llm) zoals GPT-4, Claude en Gemini, en tot [generatieve AI](referentie/begrippenlijst.md#generatieve-ai) die in staat is om tekst, afbeeldingen, audio en zelfs code te creëren.

Een belangrijk verschil met eerdere AI-modellen is dat deze nieuwe modellen niet alleen kunnen herkennen of classificeren, maar ook nieuwe dingen kunnen creëren. Dit maakt generatieve AI een van de meest veelbelovende gebieden binnen de moderne technologie. Nu kunnen we terugkijken op wat we hebben geleerd.

Naast foundation models en LLMs is er nog een belangrijke term die je tegenkomt: **MLLM** of **Multimodal Large Language Model**. Dit is eigenlijk de nieuwste generatie van AI-modellen.

**Wat is een MLLM?**
Een MLLM is een model dat kan werken met meerdere "modaliteiten" (soorten data) tegelijk. Het kan bijvoorbeeld:
- Tekst lezen en begrijpen
- Afbeeldingen analyseren en beschrijven
- Audio verwerken en transcriberen
- Video's begrijpen en samenvatten
- Al deze informatie combineren om complexe vragen te beantwoorden

> ### Quizvraag
> **Wat is een technisch kenmerk dat Large Language Models (LLM's) zoals GPT-5 onderscheidt van eerdere AI-systemen zoals expertsystemen of traditionele neurale netwerken?**
>
> A) LLM's gebruiken uitsluitend handmatig opgestelde logische regels om beslissingen te nemen.  
> B) LLM's zijn getraind op grote hoeveelheden ongestructureerde data en gebruiken miljarden parameters in een transformer-architectuur om context en betekenis in tekst te modelleren.  
> C) LLM's kunnen alleen lineair scheidbare patronen herkennen, net als het oorspronkelijke perceptron.  
> D) LLM's zijn beperkt tot het verwerken van slechts één type data, zoals alleen tekst of alleen afbeeldingen.
>
> **Juiste antwoord:** B


---
## Conclusie









